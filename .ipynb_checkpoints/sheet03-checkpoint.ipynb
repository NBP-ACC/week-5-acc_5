{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osnabr√ºck University - A&C: Computational Cognition (Summer Term 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 03: Analysis of behavioural data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in at 14:00 at **Tuesday, May 7, 2019**. If you need help (and Google and other resources were not enough), feel free to contact your tutors. Please push your results to your Github group folder.\n",
    "\n",
    "In this exercise sheet we will start to work on a real dataset from an ongoing experiment. In this experiment the participants were asked to freely explore an unknown virtual city, called Seahaven, with an interactive map. After a sufficiently long exploration the participants were tested on three different tasks: \n",
    "\n",
    "- the **absolute orientation** of a single house towards the north cardinal direction\n",
    "- the **relative orientation** between two houses \n",
    "- **pointing** from the location of one house to the other\n",
    "\n",
    "Each task type was performed in two time conditions:\n",
    "\n",
    "- **3 seconds** for spontaneous decisions\n",
    "- **infinite** response time for cognitive reasoning\n",
    "\n",
    "These measurements were repeated up to three times on different days.\n",
    "\n",
    "We will provide you with more detailed information about this experiment in the tutorial. If you are interested in more than this feel free to have a look at the paper https://www.biorxiv.org/content/10.1101/539080v1.\n",
    "\n",
    "A small side remark to the dataset: The RTs for the absolute task are exactly the same for both time conditions. This is an error that cannot be corrected on a short notice. Please keep that in mind, when you evaluate your plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 0: Peer review for sheet 02 [3 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open an issue in the repository of the groups you have to check. The title of the issue should be your group name (e.g. \"Group1). Comment on what was good and what was bad, the aesthetics and ease of reading the plots, what you would have done differently and how many points you would give them for their solutions.\n",
    "\n",
    "| * |Group 1|Group 2|Group 3|Group 4|Group 5|Group 6|Group 7|Group 8|Group 9|Group 10|Group 11|\n",
    "| ------- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ------ | ------ |\n",
    "| check solutions of group: | 11, 9 | 5, 1  | 8, 2  | 2, 7 | 10, 6 | 7, 11 | 6, 5  | 4, 3  | 3, 8  | 1, 4   | 9, 10  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Preprocessing [3 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'pandas._libs.tslibs.period.array' has no attribute '__reduce_cython__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b291922f32ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/acc/lib/python3.6/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     from pandas._libs import (hashtable as _hashtable,\n\u001b[0m\u001b[1;32m     27\u001b[0m                              \u001b[0mlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                              tslib as _tslib)\n",
      "\u001b[0;32m/anaconda3/envs/acc/lib/python3.6/site-packages/pandas/_libs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# flake8: noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from .tslibs import (\n\u001b[0m\u001b[1;32m      5\u001b[0m     iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime, Period)\n",
      "\u001b[0;32m/anaconda3/envs/acc/lib/python3.6/site-packages/pandas/_libs/tslibs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnattype\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNaT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miNaT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_null_datetimelike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnp_datetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mperiod\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeriod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIncompatibleFrequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtimestamps\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtimedeltas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdelta_to_nanoseconds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mints_to_pytimedelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/acc/lib/python3.6/site-packages/pandas/_libs/tslibs/period.cpython-36m-darwin.so\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.period\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'pandas._libs.tslibs.period.array' has no attribute '__reduce_cython__'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Preprocessing the data [1 pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all you should preprocess the data. This is an important step to avoid running into any problems when you start to analyse the data. Since we provide the Seahaven dataset as an excel file make sure to run ```pip install xlrd``` in your activated acc environment beforehand. This allows you to directly read from excel files.\n",
    "\n",
    "- Import the data of all three tasks (Absolute, Relative, Pointing) into one dataframe. Since we don't need the whole data, load only the columns \"ReactionTime\", \"AngularDiffBin\", \"Subject\", \"Task\", \"Time\", \"Answer\" and \"Measurement\".\n",
    "- Clean the dataframe of all NaNs, i.e. remove all rows where at least one element is missing. How many rows have been removed?\n",
    "- Change the values of the column \"Answer\". Replace each value \"wrong\" with 0 and each value \"correct\" with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "\n",
    "#Path to file\n",
    "FILEPATH = os.path.join(os.getcwd(), 'Seahaven_dataset.xlsx')\n",
    "\n",
    "#Print out warning message if file is not in current directiory\n",
    "if(not os.path.isfile(FILEPATH)):\n",
    "    print(\"Seahaven_dataset.xlsx file not found in current directory\")\n",
    "\n",
    "#load excel file to workbook\n",
    "wb = xlrd.open_workbook(FILEPATH)\n",
    "\n",
    "#these are the columns we want to import to dataframe\n",
    "columns = [\"ReactionTime\", \"AngularDiffBin\", \"Subject\", \"Task\", \"Time\", \"Answer\", \"Measurement\"]\n",
    "\n",
    "#dataframe from excel file, containing all three sheets (AbsoluteTask, RelativeTask, PointingTask)\n",
    "#it is a dictionary of the three dataframes, we use the dictionary to handle the dataframes easily\n",
    "dataframeDict = pd.read_excel(wb, sheet_name=wb.sheet_names() , usecols=columns )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be able to count how many lines have been removed, we need the original length of the sheets individually\n",
    "#this will be saved in the list \"removed\"\n",
    "removed = []\n",
    "for key,value in dataframeDict.items():\n",
    "    removed.append(len(dataframeDict[key]))\n",
    "\n",
    "#Inform User about the number of lines removed\n",
    "print(\"Lines removed after filtering out NaNs\")\n",
    "\n",
    "#iterating through the sheets\n",
    "for i, (key, value) in enumerate(dataframeDict.items()):\n",
    "    #removing lines containing >=0 \"NaN\" values\n",
    "    dataframeDict[key].dropna(how='any', inplace=True)\n",
    "    #replacing all \"wrong\" by 0 and all \"correct\" by 1 in column \"Answer\"\n",
    "    dataframeDict[key].Answer.replace(to_replace=[\"wrong\",\"correct\"], value=[0,1], inplace=True)\n",
    "    #now we can count and print how many lines have been removed\n",
    "    print(wb.sheet_names()[i], \":\", removed[i]-len(dataframeDict[key]))\n",
    "\n",
    "dataframeDict[\"AbsoluteTask\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Checking the distribution of the data [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the analysis techniques require normally distributed data. To get an idea on how the data looks like use the **preprocessed data** from 1.a) and plot for each task a violinplot that displays the data distribution of the RTs (note that you also have to distinguish between the two time conditions - 3sec and Infinite).\n",
    "\n",
    "- For each task (Absolute, Relative, Pointing) and time condition (3sec, Infinite) calculate the mean RT per subject.\n",
    "- Make a violinplot for each combination of task and time condition (you should end up with 6 violinplots). Make sure that the data distributions are displayed clearly and that the y-axes are uniformly scaled to make your plots comparable. You may split up the single plots for a better overview.\n",
    "- Hint: Play with the keyword inner.\n",
    "- Using your plots, what can you say about the distribution of the data? It is normally distributed? How is it skewed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#to create the plots we will first save the calculated means to a new dataframe with 7 columns:\n",
    "#the 0th column is the SubjectID, \n",
    "#columns 1&2 refer to the AbsoluteTask, 3&4 to the RelativeTask and the last two columns to the PointingTask\n",
    "#for every task there are two columns, one for each trial type\n",
    "plotColumns = ['SubID','3secA', 'infA', '3secR', 'infR', '3secP', 'infP']\n",
    "#creating the new dataframe \n",
    "plotDF = pd.DataFrame(columns=plotColumns)\n",
    "\n",
    "#a helper list to be able to add new lines to the dataframe\n",
    "row =[]\n",
    "\n",
    "#there are two types of trials\n",
    "trialTypes =['3sec', 'Infinite']\n",
    "\n",
    "#all three tasks were executed by the same amount of subjects (previously checked) thus we only need \n",
    "#to drop the duplicates of the column \"Subject\" of one of the three tasks to get a list of SubIDs\n",
    "subIDs = dataframeDict['AbsoluteTask'].Subject.drop_duplicates()\n",
    "\n",
    "#iterating through the 97 subjects to calculate the means individually\n",
    "for ID in subIDs:\n",
    "    #the first column always is the subject ID so we add the ID to the helper row\n",
    "    row.append(ID)\n",
    "    #from column 1 on the task changes every second column, thus we also need to iterate through the tasks/sheets\n",
    "    #that are saved in our dataframe dictionary\n",
    "    \n",
    "    for key,value in dataframeDict.items():\n",
    "        #grouping the current task/sheet by subject and task condition (which can be found in the column \"time\")\n",
    "        grouped =  dataframeDict[key].groupby(['Subject','Time'])\n",
    "        #for the current task and subject we have two different trials so we need a third loop that iterates through them\n",
    "        \n",
    "        for trialType in trialTypes:\n",
    "            #this will calculate the mean of the current task, subject and trialtype\n",
    "            mean = grouped.get_group((ID, trialType)).ReactionTime.mean()\n",
    "            #appending to the helper row, the order will be correct by default due to the structure of the loops\n",
    "            row.append(mean)\n",
    "            \n",
    "    #finally adding the new row to the dataset\n",
    "    plotDF = plotDF.append(pd.Series(row, index=plotColumns), ignore_index = True)\n",
    "    #clearing row for the next iteration\n",
    "    row.clear()\n",
    "\n",
    "plotDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2,ncols=3, figsize=(12,8), sharey=\"row\", sharex=\"all\")\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes[0].set_title(\"Absolute Task\")\n",
    "axes[1].set_title(\"Relative Task\")\n",
    "axes[2].set_title(\"Pointing Task\")\n",
    "\n",
    "sns.violinplot(x=None, y=plotDF['3secA'], ax=axes[0])\n",
    "sns.violinplot(x=None, y=plotDF['infA'], ax=axes[3])\n",
    "sns.violinplot(x=None, y=plotDF['3secR'], ax=axes[1])\n",
    "sns.violinplot(x=None, y=plotDF['infR'], ax=axes[4])\n",
    "sns.violinplot(x=None, y=plotDF['3secP'], ax=axes[2])\n",
    "sns.violinplot(x=None, y=plotDF['infP'], ax=axes[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that there are (extreme) outliers, that have to be removed from the **preprocessed data**.\n",
    "\n",
    "- For each task (Absolute, Relative, Pointing) and time condition (3sec, Infinite) look at the RTs and keep only the ones that are within +2 and -2 standard deviation:\n",
    "\n",
    "$ |(RT_{group1} - mean(RT_{group1}))| \\leq (2*std(RT_{group1})) $\n",
    "\n",
    "$group1$ = e.g. data of absolute task for time condition 3sec\n",
    "\n",
    "- Make again a violinplot for each combination of task and time condition (you should end up with 6 violinplots). Make sure that the data distributions are displayed clearly and that the y-axes are uniformly scaled to make your plots comparable. You may split up the single plots for a better overview.\n",
    "- How does the distribution of the data look now? Is it still skewed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(type(task)) for task in dataframeDict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframeDict['AbsoluteTask'].head())\n",
    "# take the preprocessed data and go through all the three tasks \n",
    "\n",
    "timeConditions = ['3sec', 'infinite']\n",
    "\n",
    "for task in dataframeDict:\n",
    "    # group the dataframe by the timecondition\n",
    "    grouped = dataframeDict[task].groupby(['Time'])\n",
    "    \n",
    "    # for each time condition calculate the mean \n",
    "    for timeCond in trialTypes:\n",
    "        # this will calculate the mean of the current task and the trialtype(3sec or infinite)\n",
    "        mean = grouped.get_group(timeCond).ReactionTime.mean()\n",
    "        std = grouped.get_group(timeCond).ReactionTime.std()\n",
    "        # print the mean of the group\n",
    "        print(timeCond,\"mean:\", mean)\n",
    "        print(timeCond, \"std:\", std)\n",
    "        \n",
    "        # check if the RT in each row is not an extreme outlier\n",
    "        # calculate the z-value and check if the value is more than 2 stds away form the mean\n",
    "        # it yields z = (RT-mean)/std\n",
    "        #dropByIndex = grouped.get_group(timeCond).index[((grouped.get_group(timeCond).ReactionTime - mean)/std) <= 2]\n",
    "        #grouped.get_group(timeCond).drop(dropByIndex, inplace=True)\n",
    "        dropByIndex = grouped.get_group(timeCond).index[grouped.get_group(timeCond).ReactionTime <= 2]\n",
    "        grouped.get_group(timeCond).drop(dropByIndex)\n",
    "\n",
    "print(dataframeDict['AbsoluteTask'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframeDict['AbsoluteTask'].ReactionTime <= 2)\n",
    "dataframeDict['AbsoluteTask'].index[dataframeDict['AbsoluteTask'].ReactionTime <= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframeDict['AbsoluteTask'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VICOs code\n",
    "for key in dataframe:\n",
    "    # \n",
    "    df_grouped = dataframeDict[key].groupby('Time')\n",
    "    #\n",
    "    for time,value in df_grouped:\n",
    "        mean = value.ReactionTime.mean()\n",
    "        std = value.ReactionTime.std()\n",
    "              \n",
    "        print(mean)\n",
    "        print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please make sure that you use the preprocessed data without outliers for the following assignments!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Checking for possible hypotheses [5 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Hypothesis 1 [2 pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis 1:** \"Given that subjects are limited by time, if they are faster in RT they are also less accurate.\"\n",
    "\n",
    "- Use the data of the relative task.\n",
    "- For each time condition (3sec / Infinite) calculate the mean RT and Accuracy per subject. Rename the column \"Answer\" to \"Accuracy\" (the accuracy corresponds to the averaged answer-values).\n",
    "- Make a scatterplot of the mean RT (x-axis) and the accuracy (y-axis) for the time condition \"3sec\". \n",
    "- Make a second scatterplot and add a simple linear regression line to it. Calculate the slope of the regression line (you are allowed to use scipy). \n",
    "- Considering your results, what can you say about the hypothesis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Hypothesis 2 [1 pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis 2:** \"Given that subjects have infinite time, they perform better.\"\n",
    "\n",
    "- Use the data of all three tasks.\n",
    "- For each task (Absolute, Relative, Pointing) and time condition (3sec, Infinite) calculate the accuracy per subject. Rename the column \"Answer\" to \"Accuracy\" (the accuracy corresponds to the averaged answer-values).\n",
    "- Make a pointplot of the tasks (x-axis) and the accuracy (y-axis) for both time conditions (3sec / Infinite). The y-axis should start at 0.0 and end at 1.0.\n",
    "- For better comparison print both conditions in one plot and add a line at accuracy=0.5 to check if the results are above chance.\n",
    "- Hint: Play with the keyword dodge.\n",
    "- Considering your plots, what can you say about the hypothesis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Hypothesis 3 [1 pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis 3:** \"With each subsequent session subjects get better in performance.\"\n",
    "\n",
    "- Use the data of the relative task.\n",
    "- For each time condition (3sec / Infinite) calculate the mean RT and Accuracy per subject. Rename the column \"Answer\" to \"Accuracy\" (the accuracy corresponds to the averaged answer-values).\n",
    "- Make a pointplot of the measurement (x-axis) and the accuracy (y-axis) for both time conditions (3sec / Infinite). The y-axis should start at 0.0 and end at 1.0.\n",
    "- For better comparison print both conditions in one plot and add a line at accuracy=0.5 to check if the results are above chance.\n",
    "- Hint: Play with the keyword dodge.\n",
    "- Make also a pointplot of the measurement (x-axis) and RT (y-axis) for both time conditions (3sec / Infinite).  Make sure that the data is displayed clearly. You may split up the single plots for a better overview.\n",
    "- Considering your plots, what can you say about the hypothesis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Hypothesis 4 [1 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis 4:** \"When the angular difference between houses increases subjects are more accurate in the relative task.\"\n",
    "\n",
    "- Use the data of the relative task.\n",
    "- For each angular difference (see \"AngularDiffBin\") calculate the accuracy per subject. Rename the column \"Answer\" to \"Accuracy\" (the accuracy corresponds to the averaged answer-values).\n",
    "- Make a pointplot of the angular differences (x-axis) and the accuracy (y-axis) for both time conditions (3sec / Infinite). The y-axis should start at 0.0 and end at 1.0.\n",
    "- For better comparison print both conditions in one plot and add a line at accuracy=0.5 to check if the results are above chance. Make sure that the angular differences are displayed in ascending order.\n",
    "- Hint: Play with the keyword dodge.\n",
    "- Considering your plots, what can you say about the hypothesis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: T-test [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a two-sample t-test, i.e. we compare the mean of two groups under the assumption that both are independent and normally distributed with unknown but equal variances. In this case we will look at the data of the relative task and compare the accuracies of the two time conditions (3sec / Infinte). We will ignore that there are different measurement days!\n",
    "\n",
    "- Use the data of the relative task.\n",
    "- For each time condition (3sec / Infinite) calculate the accuracy per subject. Rename the column \"Answer\" to \"Accuracy\" (the accuracy corresponds to the averaged answer-values).\n",
    "- Check if the data is normally distributed using scipy.stats.normaltest.\n",
    "\n",
    "\n",
    "- Compute the t-statistics: $ t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} $\n",
    "\n",
    "$\\bar{x}_1$: mean accuracy of all subjects for time condition \"3sec\" <br>\n",
    "$\\bar{x}_2$: mean accuracy of all subjects for time condition \"Infinite\" <br>\n",
    "$n_1$: sample size for time condition \"3sec\" <br>\n",
    "$n_2$: sample size for time condition \"Infinite\"\n",
    "\n",
    "\n",
    "- with $ s^2 = \\frac{\\sum_{i=1}^n{(x_i - \\bar{x})^2}}{n-1} $\n",
    "\n",
    "$x_i$: accuracy of subject i <br>\n",
    "$\\bar{x}$: mean accuracy of all subjects <br>\n",
    "$n$: sample size\n",
    "\n",
    "\n",
    "- Calculate the degrees of freedom: $ df = n_1 + n_2 -2 $\n",
    "- What does the p-value of a t-test tell you in general? Also explain what your calculated p-value tells you specifically (given $\\alpha = 0.05$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "df_relative_avg['accuracy'] = dataframe['RelativeTask'].groupby(['Time', 'Subject'])['Answer'].mean()\n",
    "\n",
    "dataframe['RelativeTask'].rename(columns={'Answer' : 'Accuracy'})\n",
    "\n",
    "k2, p = stats.normaltest(df_relative_avg)\n",
    "print(p)\n",
    "\n",
    "# gives you the p-value after comparing the t-statistic with the critical t value (computed internally) \n",
    "p = 1 - stats.t.cdf(t,df=df)\n",
    "\n",
    "print(\"t = \" + str(t))\n",
    "print(\"p = \" + str(2*p))\n",
    "\n",
    "# test if your calculation is correct\n",
    "t2, p2 = stats.ttest_ind(x1,x2)\n",
    "print(\"t = \" + str(t2))\n",
    "print(\"p = \" + str(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
